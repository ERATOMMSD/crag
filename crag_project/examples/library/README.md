# Usage of CRAG as a library

Here we provide an example use of CRAG as a library. More specifically, we create a testing tool that uses the code-pipeline of Cyber-physical systems (CPS) testing competition at [SBFT 2023](https://sbft23.github.io).

## Setting parameters

In our example, we want to use the search and road geometry parameters of [our tool](https://github.com/ERATOMMSD/crag-sbft2023competition) that participated in the competition. To this end we define two dictionaries `core_params` and `geometry_params`:

~~~python
core_params = {}
core_params["use_seed"] = True
core_params["seed_best"] = True
core_params["best_ratio"] = True
core_params["resample"] = True
core_params["fitness_aggregation_method"] = "minimum"
core_params["max_strength"] = 5

geometry_params = {}
geometry_params["road_section_count"] = 5
geometry_params["param_value_count"] = 5
geometry_params["max_road_scalar"] = 1.2
geometry_params["min_road_scalar"] = 0.6
geometry_params["lane_width"] = 10
geometry_params["map_size"] = 200
geometry_params["min_radius"] = 15
~~~


## Defining evaluate_function

The goal in the competition is to find road geometries that make an automated driving agent exit its prespecified lane. To characterize this goal, we define `evaluate_function` that checks whether generated roads are inside the given map, whether they are reframble to be placed in the map, and whether they are not self-intersecting. If a road passes these checks, it is passed to the competition pipeline, which returns numeric values indicating position the vehicle with respect to lane boundaries. We use these numeric values to return a single floating point number to indicate the fitness of a road.

~~~python
def evaluate_function(road):
    """Evaluate function takes a road generated by CRAG
        and returns a numeric value characterizing the
        (negative) fitness of a road. A road with a smaller
        numeric value is preferable in search. This specific
        function evaluates the road over driving agents
        specified in SBFT'23 Competition."""
    (road_points, is_in_map, is_reframable) = road
    if (not is_in_map) and (not is_reframable):
        return [1000]

    # Check self-intersections and reframe if possible
    if rg.is_likely_self_intersecting(road_points, geometry_params["lane_width"]):
        return [1000]

    if not is_reframable:
        return [1000]

    the_test = RoadTestFactory.create_road_test(road_points)
    test_outcome, description, execution_data = self.executor.execute_test(the_test)
    if execution_data:
        min_oob_distances = heapq.nsmallest(self.MIN_OOB_SAMPLE_SIZE,
                                            [getattr(x, 'oob_distance') for x in execution_data])
        # Return average of MIN_OOB_SAMPLE_SIZE number of smallest oob distances
        return [sum(min_oob_distances) / len(min_oob_distances)]
    else:
        return [100] # Road determined INVALID
~~~

### Defining budget_availability_function

Since the code-pipeline of the competition provides a budget control mechanism, we define our `budget_availability_function` by using this mechanism.

~~~python
def budget_availability_function():
    return not self.executor.is_over():
~~~

## Choosing a backend for combinatorial test-suite generation

We choose PICT as test suite generator and specify its model and seed files.

~~~python
ptsg = PictTestSuiteGenerator(get_local_pict_executable(), "PICTModel.txt", "PICTSeed.txt")
~~~

## Initializing CRAG and starting road geometry generation

`crag` module provides the class `CRAG`. We create a CRAG object by passing parameters, evaluation and budget availability functions together with the test suite generator that we picked.

~~~python
self.crag = CRAG(core_params, geometry_params, ptsg, evaluate_function, budget_availability_function)
~~~

To start generating road geometries, we use the `generate` method.

~~~python
self.crag.generate()
~~~

## Starting code-pipeline

Mode details are provided [here](https://github.com/sbft-cps-tool-competition/cps-tool-competition) on how to run the provided code inside the code-pipeline of the competition.
